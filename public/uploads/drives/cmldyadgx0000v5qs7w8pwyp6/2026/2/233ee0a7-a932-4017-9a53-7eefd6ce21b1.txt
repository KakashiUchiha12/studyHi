4.2.1 Input Port Processing and Destination-Based Forwarding
A more detailed view of input processing is shown in Figure 4.5. As just discussed,
the input port’s line-termination function and link-layer processing implement the
physical and link layers for that individual input link. The lookup performed in the
input port is central to the router’s operation—it is here that the router uses the forwarding
table to look up the output port to which an arriving packet will be forwarded
via the switching fabric. The forwarding table is either computed and updated by the
routing processor (using a routing protocol to interact with the routing processors in
other network routers) or is received from a remote SDN controller. The forwarding
table is copied from the routing processor to the line cards over a separate bus (e.g.,
a PCI bus) indicated by the dashed line from the routing processor to the input line
cards in Figure 4.4. With such a shadow copy at each line card, forwarding decisions
can be made locally, at each input port, without invoking the centralized routing processor
on a per-packet basis and thus avoiding a centralized processing bottleneck.
Let’s now consider the “simplest” case that the output port to which an incoming
packet is to be switched is based on the packet’s destination address. In the case of
32-bit IP addresses, a brute-force implementation of the forwarding table would have
one entry for every possible destination address. Since there are more than 4 billion
possible addresses, this option is totally out of the question.
Line
termination
Data link
processing
(protocol,
decapsulation)
Lookup, fowarding,
queuing Switch
fabric
Figure 4.5 ♦ Input port processing
4.2 • What ’s Inside a Router? 345
As an example of how this issue of scale can be handled, let’s suppose that our
router has four links, numbered 0 through 3, and that packets are to be forwarded to
the link interfaces as follows:
Destination Address Range Link Interface
11001000 00010111 00010000 00000000
through 0
11001000 00010111 00010111 11111111
11001000 00010111 00011000 00000000
through 1
11001000 00010111 00011000 11111111
11001000 00010111 00011001 00000000
through 2
11001000 00010111 00011111 11111111
Otherwise 3
Clearly, for this example, it is not necessary to have 4 billion entries in the router’s
forwarding table. We could, for example, have the following forwarding table with
just four entries:
Prefix Link Interface
11001000 00010111 00010 0
11001000 00010111 00011000 1
11001000 00010111 00011 2
Otherwise 3
With this style of forwarding table, the router matches a prefix of the packet’s destination
address with the entries in the table; if there’s a match, the router forwards
the packet to a link associated with the match. For example, suppose the packet’s
destination address is 11001000 00010111 00010110 10100001; because
the 21-bit prefix of this address matches the first entry in the table, the router forwards
the packet to link interface 0. If a prefix doesn’t match any of the first three entries,
then the router forwards the packet to the default interface 3. Although this sounds
simple enough, there’s a very important subtlety here. You may have noticed that it is
possible for a destination address to match more than one entry. For example, the first
24 bits of the address 11001000 00010111 00011000 10101010 match the
second entry in the table, and the first 21 bits of the address match the third entry in the
table. When there are multiple matches, the router uses the longest prefix matching
rule; that is, it finds the longest matching entry in the table and forwards the packet to
the link interface associated with the longest prefix match. We’ll see exactly why this
346 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
longest prefix-matching rule is used when we study Internet addressing in more detail
in Section 4.3.
Given the existence of a forwarding table, lookup is conceptually simple—hardware
logic just searches through the forwarding table looking for the longest
prefix match. But at Gigabit transmission rates, this lookup must be performed in
nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP datagram).
Thus, not only must lookup be performed in hardware, but techniques beyond
a simple linear search through a large table are needed; surveys of fast lookup algorithms
can be found in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must
also be paid to memory access times, resulting in designs with embedded on-chip
DRAM and faster SRAM (used as a DRAM cache) memories. In practice, Ternary
Content Addressable Memories (TCAMs) are also often used for lookup [Yu 2004].
With a TCAM, a 32-bit IP address is presented to the memory, which returns the
content of the forwarding table entry for that address in essentially constant time.
The Cisco Catalyst 6500 and 7600 Series routers and switches can hold upwards of
a million TCAM forwarding table entries [Cisco TCAM 2014].
Once a packet’s output port has been determined via the lookup, the packet
can be sent into the switching fabric. In some designs, a packet may be temporarily
blocked from entering the switching fabric if packets from other input ports are currently
using the fabric. A blocked packet will be queued at the input port and then
scheduled to cross the fabric at a later point in time. We’ll take a closer look at the
blocking, queuing, and scheduling of packets (at both input ports and output ports)
shortly. Although “lookup” is arguably the most important action in input port processing,
many other actions must be taken: (1) physical- and link-layer processing
must occur, as discussed previously; (2) the packet’s version number, checksum and
time-to-live field—all of which we’ll study in Section 4.3—must be checked and the
latter two fields rewritten; and (3) counters used for network management (such as
the number of IP datagrams received) must be updated.
Let’s close our discussion of input port processing by noting that the input port
steps of looking up a destination IP address (“match”) and then sending the packet
into the switching fabric to the specified output port (“action”) is a specific case of a
more general “match plus action” abstraction that is performed in many networked
devices, not just routers. In link-layer switches (covered in Chapter 6), link-layer
destination addresses are looked up and several actions may be taken in addition to
sending the frame into the switching fabric towards the output port. In firewalls (covered
in Chapter 8)—devices that filter out selected incoming packets—an incoming
packet whose header matches a given criteria (e.g., a combination of source/destination
IP addresses and transport-layer port numbers) may be dropped (action). In a
network address translator (NAT, covered in Section 4.3), an incoming packet whose
transport-layer port number matches a given value will have its port number rewritten
before forwarding (action). Indeed, the “match plus action” abstraction [Bosshart
2013] is both powerful and prevalent in network devices today, and is central to the
notion of generalized forwarding that we’ll study in Section 4.4.
4.2 • What ’s Inside a Router? 347
4.2.2 Switching
The switching fabric is at the very heart of a router, as it is through this fabric that
the packets are actually switched (that is, forwarded) from an input port to an output
port. Switching can be accomplished in a number of ways, as shown in Figure 4.6:
• Switching via memory. The simplest, earliest routers were traditional computers,
with switching between input and output ports being done under direct control of
the CPU (routing processor). Input and output ports functioned as traditional I/O
devices in a traditional operating system. An input port with an arriving packet
first signaled the routing processor via an interrupt. The packet was then copied
from the input port into processor memory. The routing processor then extracted
the destination address from the header, looked up the appropriate output port
in the forwarding table, and copied the packet to the output port’s buffers. In
this scenario, if the memory bandwidth is such that a maximum of B packets per
second can be written into, or read from, memory, then the overall forwarding
throughput (the total rate at which packets are transferred from input ports to output
ports) must be less than B/2. Note also that two packets cannot be forwarded
Memory
A
B
C
X
Y
Z
Memory
Key:
Input port Output port
A
X Y Z
B
C
Interconnection Network
A
B
C
X
Y
Z
Bus
Figure 4.6 ♦ Three switching techniques
348 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
at the same time, even if they have different destination ports, since only one
memory read/write can be done at a time over the shared system bus.
Some modern routers switch via memory. A major difference from early routers,
however, is that the lookup of the destination address and the storing of the packet
into the appropriate memory location are performed by processing on the input line
cards. In some ways, routers that switch via memory look very much like sharedmemory
multiprocessors, with the processing on a line card switching (writing)
packets into the memory of the appropriate output port. Cisco’s Catalyst 8500
series switches [Cisco 8500 2020] internally switches packets via a shared memory.
• Switching via a bus. In this approach, an input port transfers a packet directly to the
output port over a shared bus, without intervention by the routing processor. This is
typically done by having the input port pre-pend a switch-internal label (header) to
the packet indicating the local output port to which this packet is being transferred
and transmitting the packet onto the bus. All output ports receive the packet, but
only the port that matches the label will keep the packet. The label is then removed
at the output port, as this label is only used within the switch to cross the bus. If multiple
packets arrive to the router at the same time, each at a different input port, all
but one must wait since only one packet can cross the bus at a time. Because every
packet must cross the single bus, the switching speed of the router is limited to the
bus speed; in our roundabout analogy, this is as if the roundabout could only contain
one car at a time. Nonetheless, switching via a bus is often sufficient for routers that
operate in small local area and enterprise networks. The Cisco 6500 router [Cisco
6500 2020] internally switches packets over a 32-Gbps-backplane bus.
• Switching via an interconnection network. One way to overcome the bandwidth
limitation of a single, shared bus is to use a more sophisticated interconnection network,
such as those that have been used in the past to interconnect processors in a
multiprocessor computer architecture. A crossbar switch is an interconnection network
consisting of 2N buses that connect N input ports to N output ports, as shown
in Figure 4.6. Each vertical bus intersects each horizontal bus at a crosspoint,
which can be opened or closed at any time by the switch fabric controller (whose
logic is part of the switching fabric itself). When a packet arrives from port A and
needs to be forwarded to port Y, the switch controller closes the crosspoint at the
intersection of busses A and Y, and port A then sends the packet onto its bus, which
is picked up (only) by bus Y. Note that a packet from port B can be forwarded to
port X at the same time, since the A-to-Y and B-to-X packets use different input
and output busses. Thus, unlike the previous two switching approaches, crossbar
switches are capable of forwarding multiple packets in parallel. A crossbar
switch is non-blocking—a packet being forwarded to an output port will not be
blocked from reaching that output port as long as no other packet is currently being
forwarded to that output port. However, if two packets from two different input
ports are destined to that same output port, then one will have to wait at the input,
since only one packet can be sent over any given bus at a time. Cisco 12000 series
4.2 • What ’s Inside a Router? 349
switches [Cisco 12000 2020] use a crossbar switching network; the Cisco 7600
series can be configured to use either a bus or crossbar switch [Cisco 7600 2020].
More sophisticated interconnection networks use multiple stages of switching
elements to allow packets from different input ports to proceed towards the same
output port at the same time through the multi-stage switching fabric. See [Tobagi
1990] for a survey of switch architectures. The Cisco CRS employs a three-stage
non-blocking switching strategy. A router’s switching capacity can also be scaled
by running multiple switching fabrics in parallel. In this approach, input ports
and output ports are connected to N switching fabrics that operate in parallel. An
input port breaks a packet into K smaller chunks, and sends (“sprays”) the chunks
through K of these N switching fabrics to the selected output port, which reassembles
the K chunks back into the original packet.
4.2.3 Output Port Processing
Output port processing, shown in Figure 4.7, takes packets that have been stored
in the output port’s memory and transmits them over the output link. This includes
selecting (i.e., scheduling) and de-queuing packets for transmission, and performing
the needed link-layer and physical-layer transmission functions.
4.2.4 Where Does Queuing Occur?
If we consider input and output port functionality and the configurations shown
in Figure 4.6, it’s clear that packet queues may form at both the input ports and the
output ports, just as we identified cases where cars may wait at the inputs and outputs
of the traffic intersection in our roundabout analogy. The location and extent of
queuing
(either at the input port queues or the output port queues) will depend on
the traffic load, the relative speed of the switching fabric, and the line speed. Let’s
now consider these queues in a bit more detail, since as these queues grow large, the
router’s memory can eventually be exhausted and packet loss will occur when no
memory is available to store arriving packets. Recall that in our earlier discussions,
we said that packets were “lost within the network” or “dropped at a router.” It is here,
at these queues within a router, where such packets are actually dropped and lost.
Line
termination
Data link
processing
(protocol,
encapsulation)
Queuing (buffer
Switch management)
fabric
Figure 4.7 ♦ Output port processing
350 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
Suppose that the input and output line speeds (transmission rates) all have an
identical transmission rate of Rline packets per second, and that there are N input ports
and N output ports. To further simplify the discussion, let’s assume that all packets
have the same fixed length, and that packets arrive to input ports in a synchronous
manner. That is, the time to send a packet on any link is equal to the time to receive a
packet on any link, and during such an interval of time, either zero or one packets can
arrive on an input link. Define the switching fabric transfer rate Rswitch as the rate at
which packets can be moved from input port to output port. If Rswitch is N times faster
than Rline, then only negligible queuing will occur at the input ports. This is because
even in the worst case, where all N input lines are receiving packets, and all packets
are to be forwarded to the same output port, each batch of N packets (one packet per
input port) can be cleared through the switch fabric before the next batch arrives.
Input Queuing
But what happens if the switch fabric is not fast enough (relative to the input line
speeds) to transfer all arriving packets through the fabric without delay? In this case,
packet queuing can also occur at the input ports, as packets must join input port
queues to wait their turn to be transferred through the switching fabric to the output
port. To illustrate an important consequence of this queuing, consider a crossbar
switching fabric and suppose that (1) all link speeds are identical, (2) that one packet
can be transferred from any one input port to a given output port in the same amount
of time it takes for a packet to be received on an input link, and (3) packets are moved
from a given input queue to their desired output queue in an FCFS manner. Multiple
packets can be transferred in parallel, as long as their output ports are different. However,
if two packets at the front of two input queues are destined for the same output
queue, then one of the packets will be blocked and must wait at the input queue—the
switching fabric can transfer only one packet to a given output port at a time.
Figure 4.8 shows an example in which two packets (darkly shaded) at the front
of their input queues are destined for the same upper-right output port. Suppose that
the switch fabric chooses to transfer the packet from the front of the upper-left queue.
In this case, the darkly shaded packet in the lower-left queue must wait. But not only
must this darkly shaded packet wait, so too must the lightly shaded packet that is
queued behind that packet in the lower-left queue, even though there is no contention
for the middle-right output port (the destination for the lightly shaded packet).
This phenomenon is known as head-of-the-line (HOL) blocking in an input-queued
switch—a queued packet in an input queue must wait for transfer through the fabric
(even though its output port is free) because it is blocked by another packet at the
head of the line. [Karol 1987] shows that due to HOL blocking, the input queue will
grow to unbounded length (informally, this is equivalent to saying that significant
packet loss will occur) under certain assumptions as soon as the packet arrival rate
on the input links reaches only 58 percent of their capacity. A number of solutions to
HOL blocking are discussed in [McKeown 1997].
4.2 • What ’s Inside a Router? 351
Output Queuing
Let’s next consider whether queuing can occur at a switch’s output ports. Suppose that
Rswitch is again N times faster than Rline and that packets arriving at each of the N input
ports are destined to the same output port. In this case, in the time it takes to send a
single packet onto the outgoing link, N new packets will arrive at this output port
(one from each of the N input ports). Since the output port can transmit only a single
packet in a unit of time (the packet transmission time), the N arriving packets will
have to queue (wait) for transmission over the outgoing link. Then N more packets
can possibly arrive in the time it takes to transmit just one of the N packets that had
just previously been queued. And so on. Thus, packet queues can form at the output
ports even when the switching fabric is N times faster than the port line speeds.
Eventually, the number of queued packets can grow large enough to exhaust available
memory at the output port.
Switch
fabric
Output port contention at time t—
one dark packet can be transferred
Light blue packet experiences HOL blocking
Switch
fabric
Key:
destined for upper output
port
destined for middle output
port
destined for lower output
port
Figure 4.8 ♦ HOL blocking at and input-queued switch
352 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
When there is not enough memory to buffer an incoming packet, a decision must
be made to either drop the arriving packet (a policy known as drop-tail) or remove
one or more already-queued packets to make room for the newly arrived packet. In
some cases, it may be advantageous to drop (or mark the header of) a packet before
the buffer is full in order to provide a congestion signal to the sender. This marking
could be done using the Explicit Congestion Notification bits that we studied in
Section 3.7.2. A number of proactive packet-dropping and -marking policies (which
collectively have become known as active queue management (AQM) algorithms)
have been proposed and analyzed [Labrador 1999, Hollot 2002]. One of the most
widely studied and implemented AQM algorithms is the Random Early Detection
(RED) algorithm [Christiansen 2001]. More recent AQM policies include PIE (the
Proportional Integral controller Enhanced [RFC 8033]), and CoDel [Nichols 2012].
Output port queuing is illustrated in Figure 4.9. At time t, a packet has arrived
at each of the incoming input ports, each destined for the uppermost outgoing port.
Assuming identical line speeds and a switch operating at three times the line speed, one
time unit later (that is, in the time needed to receive or send a packet), all three original
packets have been transferred to the outgoing port and are queued awaiting transmission.
In the next time unit, one of these three packets will have been transmitted over the
outgoing link. In our example, two new packets have arrived at the incoming side of the
Switch
fabric
Output port contention at time t
One packet time later
Switch
fabric
Figure 4.9 ♦ Output port queuing
4.2 • What ’s Inside a Router? 353
switch; one of these packets is destined for this uppermost output port. A consequence
of such queuing is that a packet scheduler at the output port must choose one packet,
among those queued, for transmission—a topic we’ll cover in the following section.
How Much Buffering Is “Enough?”
Our study above has shown how a packet queue forms when bursts of packets arrive
at a router’s input or (more likely) output port, and the packet arrival rate temporarily
exceeds the rate at which packets can be forwarded. The longer the amount of time
that this mismatch persists, the longer the queue will grow, until eventually a port’s
buffers become full and packets are dropped. One natural question is how much
buffering should be provisioned at a port. It turns out the answer to this question is
much more complicated than one might imagine and can teach us quite a bit about
the subtle interaction among congestion-aware senders at the network’s edge and the
network core!
For many years, the rule of thumb [RFC 3439] for buffer sizing was that the
amount of buffering (B) should be equal to an average round-trip time (RTT, say
250 msec) times the link capacity (C). Thus, a 10-Gbps link with an RTT of 250 msec
would need an amount of buffering equal to B = RTT # C = 2.5 Gbits of buffers.
This result was based on an analysis of the queuing dynamics of a relatively
small number of TCP flows [Villamizar 1994]. More recent theoretical and experimental
efforts [Appenzeller 2004], however, suggest that when a large number of
independent TCP flows (N) pass through a link, the amount of buffering needed is
B = RTT # C>2N. In core networks, where a large number of TCP flows typically
pass through large backbone router links, the value of N can be large, with
the decrease in needed buffer size becoming quite significant. [Appenzeller 2004;
Wischik 2005; Beheshti 2008] provide very readable discussions of the buffer-sizing
problem from a theoretical, implementation, and operational standpoint.
It’s temping to think that more buffering must be better—larger buffers would
allow a router to absorb larger fluctuations in the packet arrival rate, thereby decreasing
the router’s packet loss rate. But larger buffers also mean potentially longer
queuing delays. For gamers and for interactive teleconferencing users, tens of milliseconds
count. Increasing the amount of per-hop buffer by a factor of 10 to decrease
packet loss could increase the end-end delay by a factor of 10! Increased RTTs also
make TCP senders less responsive and slower to respond to incipient congestion and/
or packet loss. These delay-based considerations show that buffering is a doubleedged
sword—buffering can be used to absorb short-term statistical fluctuations in
traffic but can also lead to increased delay and the attendant concerns. Buffering is
a bit like salt—just the right amount of salt makes food better, but too much makes
it inedible!
In the discussion above, we’ve implicitly assumed that many independent senders
are competing for bandwidth and buffers at a congested link. While this is probably
an excellent assumption for routers within the network core, at the network edge
354 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
this may not hold. Figure 4.10(a) shows a home router sending TCP segments to a
remote game server. Following [Nichols 2012], suppose that it takes 20 ms to transmit
a packet (containing a gamer’s TCP segment), that there are negligible queuing
delays elsewhere on the path to the game server, and that the RTT is 200 ms. As
shown in Figure 4.10(b), suppose that at time t = 0, a burst of 25 packets arrives to
the queue. One of these queued packets is then transmitted once every 20 ms, so that
at t = 200 msec, the first ACK arrives, just as the 21st packet is being transmitted.
This ACK arrival causes the TCP sender to send another packet, which is queued at
the outgoing link of the home router. At t = 220, the next ACK arrives, and another
TCP segment is released by the gamer and is queued, as the 22nd packet is being
transmitted, and so on. You should convince yourself that in this scenario, ACK
clocking results in a new packet arriving at the queue every time a queued packet
is sent, resulting in queue size at the home router’s outgoing link that is always five
packets! That is, the end-end-pipe is full (delivering packets to the destination at the
path bottleneck rate of one packet every 20 ms), but the amount of queuing delay is
constant and persistent. As a result, the gamer is unhappy with the delay, and the parent
(who even knows wireshark!) is confused because he or she doesn’t understand
why delays are persistent and excessively long, even when there is no other traffic
on the home network.
This scenario above of long delay due to persistent buffering is known as bufferbloat
and illustrates that not only is throughput important, but also minimal delay
is important as well [Kleinrock 2018], and that the interaction among senders at the
network edge and queues within the network can indeed be complex and subtle. The
DOCSIS 3.1 standard for cable networks that we will study in Chapter 6, recently
added a specific AQM mechanism [RFC 8033, RFC 8034] to combat bufferbloat,
while preserving bulk throughput performance.
250 ms RTT
Time (ms)
Queue length
0 200
25
5
a. b.
Home Network
Internet
Figure 4.10 ♦ Bufferbloat: persistent queues
4.2 • What ’s Inside a Router? 355
4.2.5 Packet Scheduling
Let’s now return to the question of determining the order in which queued packets are
transmitted over an outgoing link. Since you yourself have undoubtedly had to wait in
long lines on many occasions and observed how waiting customers are served, you’re
no doubt familiar with many of the queuing disciplines commonly used in routers.
There is first-come-first-served (FCFS, also known as first-in-first-out, FIFO). The
British are famous for patient and orderly FCFS queuing at bus stops and in the marketplace
(“Oh, are you queuing?”). Other countries operate on a priority basis, with
one class of waiting customers given priority service over other waiting customers.
There is also round-robin queuing, where customers are again divided into classes
(as in priority queuing) but each class of customer is given service in turn.
First-in-First-Out (FIFO)
Figure 4.11 shows the queuing model abstraction for the FIFO link-scheduling discipline.
Packets arriving at the link output queue wait for transmission if the link is
currently busy transmitting another packet. If there is not sufficient buffering space
to hold the arriving packet, the queue’s packet-discarding policy then determines
whether the packet will be dropped (lost) or whether other packets will be removed
from the queue to make space for the arriving packet, as discussed above. In our
discussion below, we’ll ignore packet discard. When a packet is completely transmitted
over the outgoing link (that is, receives service) it is removed from the queue.
The FIFO (also known as first-come-first-served, or FCFS) scheduling discipline
selects packets for link transmission in the same order in which they arrived
at the output link queue. We’re all familiar with FIFO queuing from service centers,
where arriving customers join the back of the single waiting line, remain in order, and
are then served when they reach the front of the line. Figure 4.12 shows the FIFO queue
in operation. Packet arrivals are indicated by numbered arrows above the upper timeline,
with the number indicating the order in which the packet arrived. Individual packet
departures are shown below the lower timeline. The time that a packet spends in service
(being transmitted) is indicated by the shaded rectangle between the two timelines. In
Arrivals Departures
Queue
(waiting area)
Link
(server)
Figure 4.11 ♦ FIFO queuing abstraction
356 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
our examples here, let’s assume that each packet takes three units of time to be transmitted.
Under the FIFO discipline, packets leave in the same order in which they arrived.
Note that after the departure of packet 4, the link remains idle (since packets 1 through
4 have been transmitted and removed from the queue) until the arrival of packet 5.
Priority Queuing
Under priority queuing, packets arriving at the output link are classified into priority
classes upon arrival at the queue, as shown in Figure 4.13. In practice, a network
operator may configure a queue so that packets carrying network management information
(for example, as indicated by the source or destination TCP/UDP port number)
receive priority over user traffic; additionally, real-time voice-over-IP packets
might receive priority over non-real-time traffic such e-mail packets. Each priority
class typically has its own queue. When choosing a packet to transmit, the priority
Time
Arrivals
Departures
Packet
in service
Time
1
1 2 3 4 5
2 3
1
t = 0 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
2 3 4 5
4 5
Figure 4.12 ♦ The FIFO queue in operation
Arrivals Departures
Low-priority queue
(waiting area)
Classify
High-priority queue
(waiting area)
Link
(server)
Figure 4.13 ♦ The priority queuing model
4.2 • What ’s Inside a Router? 357
NET NEUTRALITY
We’ve seen that packet scheduling mechanisms (e.g., priority traffic scheduling disciplines
such a strict priority, and WFQ) can be used to provide different levels of service to different
“classes” of traffic. The definition of what precisely constitutes a “class” of traffic is up
to an ISP to decide, but could be potentially based on any set of fields in the IP datagram
header. For example, the port field in the IP datagram header could be used to classify
datagrams according to the “well-know service” associated with that port: SNMP network
management datagram (port 161) might be assigned to a higher priority class than an
IMAP e-mail protocol (ports 143, or 993) datagram and therefore receive better service.
An ISP could also potentially use a datagram’s source IP address to provide priority to
datagrams being sent by certain companies (who have presumably paid the ISP for this
privilege) over datagrams being sent from other companies (who have not paid); an ISP
PRINCIPLES IN PRACTICE
queuing discipline will transmit a packet from the highest priority class that has a
nonempty queue (that is, has packets waiting for transmission). The choice among
packets in the same priority class is typically done in a FIFO manner.
Figure 4.14 illustrates the operation of a priority queue with two priority classes.
Packets 1, 3, and 4 belong to the high-priority class, and packets 2 and 5 belong to
the low-priority class. Packet 1 arrives and, finding the link idle, begins transmission.
During the transmission of packet 1, packets 2 and 3 arrive and are queued in the lowand
high-priority queues, respectively. After the transmission of packet 1, packet 3
(a high-priority packet) is selected for transmission over packet 2 (which, even
though it arrived earlier, is a low-priority packet). At the end of the transmission of
packet 3, packet 2 then begins transmission. Packet 4 (a high-priority packet) arrives
during the transmission of packet 2 (a low-priority packet). Under a non-preemptive
priority queuing discipline, the transmission of a packet is not interrupted once it
Arrivals
Departures
Packet
in service
1
1 3 2 4 5
2 3
1
4 5
Time
Time
t = 0 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
3 2 4 5
Figure 4.14 ♦ The priority queue in operation
358 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
could even block traffic with a source IP address in a given company, or country. There
are many mechanisms that would allow an ISP to provide different levels of service to different
classes of traffic. The real question is what policies and laws determine what an ISP
can actually do. Of course, these laws will vary by country; see [Smithsonian 2017] for a
brief survey. Here, we’ll briefly consider US policy on what has come to be known as “net
neutrality.”
The term “net neutrality” doesn’t have a precise decision, but the March 2015
Order on Protecting and Promoting an Open Internet [FCC 2015] by the US Federal
Communications Commission provides three “clear, bright line” rules that are now often
associated with net neutrality:
• “No Blocking. . . . A person engaged in the provision of broadband Internet access
service, . . . shall not block lawful content, applications, services, or non-harmful
devices, subject to reasonable network management.”
• “No Throttling. . . . A person engaged in the provision of broadband Internet
access service, . . . shall not impair or degrade lawful Internet traffic on the basis of
Internet content, application, or service, or use of a non-harmful device, subject to reasonable
network management.”
• “No Paid Prioritization. . . . A person engaged in the provision of broadband
Internet access service, . . . shall not engage in paid prioritization. “Paid prioritization”
refers to the management of a broadband provider’s network to directly or indirectly
favor some traffic over other traffic, including through use of techniques such as traffic
shaping, prioritization, resource reservation, or other forms of preferential traffic management,
. . .”
Quite interestingly, before the Order, ISP behaviors violating the first two of these rules
had been observed [Faulhaber 2012]. In 2005, an ISP in North Carolina agreed to stop
its practice of blocking its customers from using Vonage, a voice-over-IP service that competed
with its own telephone service. In 2007, Comcast was judged to be interfering with
BitTorrent P2P traffic by internally creating and sending TCP RST packets to BitTorrent senders
and receivers, which caused them to close their BitTorrent connection [FCC 2008].
Both sides of the net neutrality debate have been argued strenuously, mostly focused
on the extent to which net neutrality provides benefits to customers, while at the same
time promoting innovation. See [Peha 2006, Faulhaber 2012, Economides 2017,
Madhyastha 2017].
The 2015 FCC Order on Protecting and Promoting an Open Internet, which banned
ISPs from blocking, throttling, or providing paid prioritizing, was superseded by the 2017
FCC Restoring Internet Freedom Order, [FCC 2017] which rolled back these prohibitions
and focused instead on ISP transparency. With so much interest and so many changes,
it’s probably safe to say we aren’t close to having seen the final chapter written on net
neutrality in the United States, or elsewhere.
4.2 • What ’s Inside a Router? 359
has begun. In this case, packet 4 queues for transmission and begins being transmitted
after the transmission of packet 2 is completed.
Round Robin and Weighted Fair Queuing (WFQ)
Under the round robin queuing discipline, packets are sorted into classes as with
priority queuing. However, rather than there being a strict service priority among
classes, a round robin scheduler alternates service among the classes. In the simplest
form of round robin scheduling, a class 1 packet is transmitted, followed by a class
2 packet, followed by a class 1 packet, followed by a class 2 packet, and so on. A
so-called work-conserving queuing discipline will never allow the link to remain
idle whenever there are packets (of any class) queued for transmission. A workconserving
round robin discipline that looks for a packet of a given class but finds
none will immediately check the next class in the round robin sequence.
Figure 4.15 illustrates the operation of a two-class round robin queue. In this
example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5 belong to the
second class. Packet 1 begins transmission immediately upon arrival at the output
queue. Packets 2 and 3 arrive during the transmission of packet 1 and thus queue for
transmission. After the transmission of packet 1, the link scheduler looks for a class 2
packet and thus transmits packet 3. After the transmission of packet 3, the scheduler
looks for a class 1 packet and thus transmits packet 2. After the transmission of packet 2,
packet 4 is the only queued packet; it is thus transmitted immediately after packet 2.
A generalized form of round robin queuing that has been widely implemented
in routers is the so-called weighted fair queuing (WFQ) discipline [Demers 1990;
Parekh 1993. WFQ is illustrated in Figure 4.16. Here, arriving packets are classified
and queued in the appropriate per-class waiting area. As in round robin scheduling,
a WFQ scheduler will serve classes in a circular manner—first serving class 1, then
serving class 2, then serving class 3, and then (assuming there are three classes)
repeating the service pattern. WFQ is also a work-conserving queuing discipline and
Arrivals
Packet
in service
1
1 3 2 4 5
2 3
1 3 2 4 5
4 5
Departures
Time
Time
t = 0 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
Figure 4.15 ♦ The two-class robin queue in operation
360 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
thus will immediately move on to the next class in the service sequence when it finds
an empty class queue.
WFQ differs from round robin in that each class may receive a differential amount
of service in any interval of time. Specifically, each class, i, is assigned a weight, wi.
Under WFQ, during any interval of time during which there are class i packets to send,
class i will then be guaranteed to receive a fraction of service equal to wi>(gwj), where
the sum in the denominator is taken over all classes that also have packets queued for
transmission. In the worst case, even if all classes have queued packets, class i will still
be guaranteed to receive a fraction wi >(gwj) of the bandwidth, where in this worst
case the sum in the denominator is over all classes. Thus, for a link with transmission
rate R, class i will always achieve a throughput of at least R # wi >(gwj). Our description
of WFQ has been idealized, as we have not considered the fact that packets are
discrete and a packet’s transmission will not be interrupted to begin transmission of
another packet; [Demers 1990; Parekh 1993] discuss this packetization issue.
4.3 The Internet Protocol (IP): IPv4, Addressing,
IPv6, and More
Our study of the network layer thus far in Chapter 4—the notion of the data and control
plane component of the network layer, our distinction between forwarding and
routing, the identification of various network service models, and our look inside a
router—have often been without reference to any specific computer network architecture
or protocol. In this section, we’ll focus on key aspects of the network layer on
today’s Internet and the celebrated Internet Protocol (IP).
There are two versions of IP in use today. We’ll first examine the widely
deployed IP protocol version 4, which is usually referred to simply as IPv4 [RFC
791] in Section 4.3.1. We’ll examine IP version 6 [RFC 2460; RFC 4291], which has
Classify
Arrivals Departures
w1
w2
w3 Link
Figure 4.16 ♦ Weighted fair queuing
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 361
been proposed to replace IPv4, in Section 4.3.4. In between, we’ll primarily cover
Internet addressing—a topic that might seem rather dry and detail-oriented but we’ll
see is crucial to understanding how the Internet’s network layer works. To master IP
addressing is to master the Internet’s network layer itself!
4.3.1 IPv4 Datagram Format
Recall that the Internet’s network-layer packet is referred to as a datagram. We begin
our study of IP with an overview of the syntax and semantics of the IPv4 datagram.
You might be thinking that nothing could be drier than the syntax and semantics of a
packet’s bits. Nevertheless, the datagram plays a central role in the Internet—every
networking student and professional needs to see it, absorb it, and master it. (And
just to see that protocol headers can indeed be fun to study, check out [Pomeranz
2010]). The IPv4 datagram format is shown in Figure 4.17. The key fields in the IPv4
datagram are the following:
• Version number. These 4 bits specify the IP protocol version of the datagram.
By looking at the version number, the router can determine how to interpret the
remainder of the IP datagram. Different versions of IP use different datagram
formats. The datagram format for IPv4 is shown in Figure 4.17. The datagram
format for the new version of IP (IPv6) is discussed in Section 4.3.4.
• Header length. Because an IPv4 datagram can contain a variable number of
options (which are included in the IPv4 datagram header), these 4 bits are needed
Version Header Type of service
length
Upper-layer
protocol
16-bit Identifier
Time-to-live
Flags 13-bit Fragmentation offset
Datagram length (bytes)
Header checksum
32 bits
32-bit Source IP address
32-bit Destination IP address
Options (if any)
Data
Figure 4.17 ♦ IPv4 datagram format
362 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
to determine where in the IP datagram the payload (for example, the transportlayer
segment being encapsulated in this datagram) actually begins. Most IP datagrams
do not contain options, so the typical IP datagram has a 20-byte header.
• Type of service. The type of service (TOS) bits were included in the IPv4 header
to allow different types of IP datagrams to be distinguished from each other. For
example, it might be useful to distinguish real-time datagrams (such as those
used by an IP telephony application) from non-real-time traffic (e.g., FTP). The
specific
level of service to be provided is a policy issue determined and configured
by the network administrator for that router. We also learned in Section 3.7.2
that two of the TOS bits are used for Explicit Congestion Notification.
• Datagram length. This is the total length of the IP datagram (header plus data), measured
in bytes. Since this field is 16 bits long, the theoretical maximum size of the IP
datagram is 65,535 bytes. However, datagrams are rarely larger than 1,500 bytes, which
allows an IP datagram to fit in the payload field of a maximally sized Ethernet frame.
• Identifier, flags, fragmentation offset. These three fields have to do with so-called
IP fragmentation, when a large IP datagram is broken into several smaller IP datagrams
which are then forwarded independently to the destination, where they are
reassembled before their payload data (see below) is passed up to the transport layer
at the destination host. Interestingly, the new version of IP, IPv6, does not allow for
fragmentation. We’ll not cover fragmentation here; but readers can find a detailed
discussion online, among the “retired” material from earlier versions of this book.
• Time-to-live. The time-to-live (TTL) field is included to ensure that datagrams
do not circulate forever (due to, for example, a long-lived routing loop) in the
network. This field is decremented by one each time the datagram is processed by
a router. If the TTL field reaches 0, a router must drop that datagram.
• Protocol. This field is typically used only when an IP datagram reaches its final
destination. The value of this field indicates the specific transport-layer protocol
to which the data portion of this IP datagram should be passed. For example, a
value of 6 indicates that the data portion is passed to TCP, while a value of 17 indicates
that the data is passed to UDP. For a list of all possible values, see [IANA
Protocol Numbers 2016]. Note that the protocol number in the IP datagram has
a role that is analogous to the role of the port number field in the transport-layer
segment. The protocol number is the glue that binds the network and transport
layers together, whereas the port number is the glue that binds the transport and
application layers together. We’ll see in Chapter 6 that the link-layer frame also
has a special field that binds the link layer to the network layer.
• Header checksum. The header checksum aids a router in detecting bit errors in a
received IP datagram. The header checksum is computed by treating each 2 bytes
in the header as a number and summing these numbers using 1s complement arithmetic.
As discussed in Section 3.3, the 1s complement of this sum, known as
the Internet checksum, is stored in the checksum field. A router computes the
header checksum for each received IP datagram and detects an error condition if
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 363
the checksum carried in the datagram header does not equal the computed checksum.
Routers typically discard datagrams for which an error has been detected.
Note that the checksum must be recomputed and stored again at each router, since
the TTL field, and possibly the options field as well, will change. An interesting
discussion of fast algorithms for computing the Internet checksum is [RFC 1071].
A question often asked at this point is, why does TCP/IP perform error checking at
both the transport and network layers? There are several reasons for this repetition.
First, note that only the IP header is checksummed at the IP layer, while the TCP/
UDP checksum is computed over the entire TCP/UDP segment. Second, TCP/
UDP and IP do not necessarily both have to belong to the same protocol stack.
TCP can, in principle, run over a different network-layer protocol (for example,
ATM) [Black 1995]) and IP can carry data that will not be passed to TCP/UDP.
• Source and destination IP addresses. When a source creates a datagram, it inserts
its IP address into the source IP address field and inserts the address of the ultimate
destination into the destination IP address field. Often the source host determines
the destination address via a DNS lookup, as discussed in Chapter 2. We’ll
discuss IP addressing in detail in Section 4.3.2.
• Options. The options fields allow an IP header to be extended. Header options
were meant to be used rarely—hence the decision to save overhead by not including
the information in options fields in every datagram header. However, the
mere existence of options does complicate matters—since datagram headers can
be of variable length, one cannot determine a priori where the data field will start.
Also, since some datagrams may require options processing and others may not,
the amount of time needed to process an IP datagram at a router can vary greatly.
These considerations become particularly important for IP processing in highperformance
routers and hosts. For these reasons and others, IP options were not
included in the IPv6 header, as discussed in Section 4.3.4.
• Data (payload). Finally, we come to the last and most important field—the raison
d’etre for the datagram in the first place! In most circumstances, the data field of
the IP datagram contains the transport-layer segment (TCP or UDP) to be delivered
to the destination. However, the data field can carry other types of data, such
as ICMP messages (discussed in Section 5.6).
Note that an IP datagram has a total of 20 bytes of header (assuming no options).
If the datagram carries a TCP segment, then each datagram carries a total of
40 bytes of header (20 bytes of IP header plus 20 bytes of TCP header) along with
the application-layer message.
4.3.2 IPv4 Addressing
We now turn our attention to IPv4 addressing. Although you may be thinking that
addressing must be a straightforward topic, hopefully by the end of this section you’ll
be convinced that Internet addressing is not only a juicy, subtle, and interesting topic
364 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
but also one that is of central importance to the Internet. An excellent treatment of
IPv4 addressing can be found in the first chapter in [Stewart 1999].
Before discussing IP addressing, however, we’ll need to say a few words about
how hosts and routers are connected into the Internet. A host typically has only a
single link into the network; when IP in the host wants to send a datagram, it does
so over this link. The boundary between the host and the physical link is called
an interface. Now consider a router and its interfaces. Because a router’s job is to
receive a datagram on one link and forward the datagram on some other link, a router
necessarily has two or more links to which it is connected. The boundary between the
router and any one of its links is also called an interface. A router thus has multiple
interfaces, one for each of its links. Because every host and router is capable of sending
and receiving IP datagrams, IP requires each host and router interface to have
its own IP address. Thus, an IP address is technically associated with an interface,
rather than with the host or router containing that interface.
Each IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total
of 232 (or approximately 4 billion) possible IP addresses. These addresses are typically
written in so-called dotted-decimal notation, in which each byte of the address
is written in its decimal form and is separated by a period (dot) from other bytes in
the address. For example, consider the IP address 193.32.216.9. The 193 is the decimal
equivalent of the first 8 bits of the address; the 32 is the decimal equivalent of
the second 8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary
notation is
11000001 00100000 11011000 00001001
Each interface on every host and router in the global Internet must have an IP address
that is globally unique (except for interfaces behind NATs, as discussed in Section 4.3.3).
These addresses cannot be chosen in a willy-nilly manner, however. A portion of
an interface’s IP address will be determined by the subnet to which it is connected.
Figure 4.18 provides an example of IP addressing and interfaces. In this figure,
one router (with three interfaces) is used to interconnect seven hosts. Take a close
look at the IP addresses assigned to the host and router interfaces, as there are several
things to notice. The three hosts in the upper-left portion of Figure 4.18, and
the router interface to which they are connected, all have an IP address of the form
223.1.1.xxx. That is, they all have the same leftmost 24 bits in their IP address. These
four interfaces are also interconnected to each other by a network that contains no
routers. This network could be interconnected by an Ethernet LAN, in which case
the interfaces would be interconnected by an Ethernet switch (as we’ll discuss in
Chapter 6), or by a wireless access point (as we’ll discuss in Chapter 7). We’ll represent
this routerless network connecting these hosts as a cloud for now, and dive into
the internals of such networks in Chapters 6 and 7.
In IP terms, this network interconnecting three host interfaces and one router
interface forms a subnet [RFC 950]. (A subnet is also called an IP network or simply
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 365
a network in the Internet literature.) IP addressing assigns an address to this subnet:
223.1.1.0/24, where the /24 (“slash-24”) notation, sometimes known as a subnet
mask, indicates that the leftmost 24 bits of the 32-bit quantity define the subnet
address. The 223.1.1.0/24 subnet thus consists of the three host interfaces (223.1.1.1,
223.1.1.2, and 223.1.1.3) and one router interface (223.1.1.4). Any additional hosts
attached to the 223.1.1.0/24 subnet would be required to have an address of the form
223.1.1.xxx. There are two additional subnets shown in Figure 4.18: the 223.1.2.0/24
network and the 223.1.3.0/24 subnet. Figure 4.19 illustrates the three IP subnets present
in Figure 4.18.
The IP definition of a subnet is not restricted to Ethernet segments that connect
multiple hosts to a router interface. To get some insight here, consider Figure 4.20,
which shows three routers that are interconnected with each other by point-to-point
links. Each router has three interfaces, one for each point-to-point link and one for
the broadcast link that directly connects the router to a pair of hosts. What subnets
are present here? Three subnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are
similar to the subnets we encountered in Figure 4.18. But note that there are three
additional subnets in this example as well: one subnet, 223.1.9.0/24, for the interfaces
that connect routers R1 and R2; another subnet, 223.1.8.0/24, for the interfaces
that connect routers R2 and R3; and a third subnet, 223.1.7.0/24, for the interfaces
that connect routers R3 and R1. For a general interconnected system of routers and
hosts, we can use the following recipe to define the subnets in the system:
223.1.1.1
223.1.2.1
223.1.2.2
223.1.1.2
223.1.1.4 223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1 223.1.3.2
Figure 4.18 ♦ Interface addresses and subnets
366 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
To determine the subnets, detach each interface from its host or router, creating
islands of isolated networks, with interfaces terminating the end points of the
isolated networks. Each of these isolated networks is called a subnet.
If we apply this procedure to the interconnected system in Figure 4.20, we get six
islands or subnets.
From the discussion above, it’s clear that an organization (such as a company or
academic institution) with multiple Ethernet segments and point-to-point links will
have multiple subnets, with all of the devices on a given subnet having the same subnet
address. In principle, the different subnets could have quite different subnet addresses.
In practice, however, their subnet addresses often have much in common. To understand
why, let’s next turn our attention to how addressing is handled in the global Internet.
The Internet’s address assignment strategy is known as Classless Interdomain
Routing (CIDR—pronounced cider) [RFC 4632]. CIDR generalizes the notion of
subnet addressing. As with subnet addressing, the 32-bit IP address is divided into
two parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the
number of bits in the first part of the address.
The x most significant bits of an address of the form a.b.c.d/x constitute the
network portion of the IP address, and are often referred to as the prefix (or network
prefix) of the address. An organization is typically assigned a block of contiguous
addresses, that is, a range of addresses with a common prefix (see the Principles in
Practice feature). In this case, the IP addresses of devices within the organization
will share the common prefix. When we cover the Internet’s BGP routing protocol in
223.1.1.0/24
223.1.2.0/24
223.1.3.0/24
Figure 4.19 ♦ Subnet addresses
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 367
Section 5.4, we’ll see that only these x leading prefix bits are considered by routers
outside the organization’s network. That is, when a router outside the organization
forwards a datagram whose destination address is inside the organization, only the
leading x bits of the address need be considered. This considerably reduces the size
of the forwarding table in these routers, since a single entry of the form a.b.c.d/x will
be sufficient to forward packets to any destination within the organization.
The remaining 32-x bits of an address can be thought of as distinguishing among the
devices within the organization, all of which have the same network prefix. These are
the bits that will be considered when forwarding packets at routers within the organization.
These lower-order bits may (or may not) have an additional subnetting structure,
such as that discussed above. For example, suppose the first 21 bits of the CIDRized
address a.b.c.d/21 specify the organization’s network prefix and are common to the IP
addresses of all devices in that organization. The remaining 11 bits then identify the
specific hosts in the organization. The organization’s internal structure might be such
that these 11 rightmost bits are used for subnetting within the organization, as discussed
above. For example, a.b.c.d/24 might refer to a specific subnet within the organization.
Before CIDR was adopted, the network portions of an IP address were constrained
to be 8, 16, or 24 bits in length, an addressing scheme known as classful addressing,
223.1.8.1 223.1.8.0
223.1.9.1 223.1.7.1
223.1.2.6
223.1.2.1 223.1.2.2 223.1.3.1 223.1.3.2
223.1.1.3
223.1.9.2 223.1.7.0
223.1.3.27
223.1.1.1 223.1.1.4
R1
R2 R3
Figure 4.20 ♦ Three routers interconnecting six subnets
368 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
since subnets with 8-, 16-, and 24-bit subnet addresses were known as class A, B, and
C networks, respectively. The requirement that the subnet portion of an IP address be
exactly 1, 2, or 3 bytes long turned out to be problematic for supporting the rapidly
growing number of organizations with small and medium-sized subnets. A class C
(/24) subnet could accommodate only up to 28 2 2 5 254 hosts (two of the 28 5 256
addresses are reserved for special use)—too small for many organizations. However, a
class B (/16) subnet, which supports up to 65,634 hosts, was too large. Under classful
addressing, an organization with, say, 2,000 hosts was typically allocated a class B
(/16) subnet address. This led to a rapid depletion of the class B address space and
poor utilization of the assigned address space. For example, the organization that
used a class B address for its 2,000 hosts was allocated enough of the address space
for up to 65,534 interfaces—leaving more than 63,000 addresses that could not be
used by other organizations.
This example of an ISP that connects eight organizations to the Internet nicely illustrates how
carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure 4.21,
that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it should
be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The rest of
the world need not know that within the address block 200.23.16.0/20 there are in fact
eight other organizations, each with its own subnets. This ability to use a single prefix to
advertise multiple networks is often referred to as address aggregation (also route
aggregation or route summarization).
Address aggregation works extremely well when addresses are allocated in blocks
to ISPs and then from ISPs to client organizations. But what happens when addresses
are not allocated in such a hierarchical manner? What would happen, for example, if
Fly-By-Night-ISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet
through its subsidiary ISPs-R-Us? As shown in Figure 4.21, the subsidiary ISPs-R-Us owns
the address block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately
outside of this address block. What should be done here? Certainly, Organization 1 could
renumber all of its routers and hosts to have addresses within the ISPs-R-Us address block.
But this is a costly solution, and Organization 1 might well be reassigned to another
subsidiary in the future. The solution typically adopted is for Organization 1 to keep its
IP addresses in 200.23.18.0/23. In this case, as shown in Figure 4.22, Fly-By-Night-ISP
continues to advertise the address block 200.23.16.0/20 and ISPs-R-Us continues to
advertise 199.31.0.0/16. However, ISPs-R-Us now also advertises the block of addresses
for Organization 1, 200.23.18.0/23. When other routers in the larger Internet see the
address blocks 200.23.16.0/20 (from Fly-By-Night-ISP) and 200.23.18.0/23 (from ISPs-
R-Us) and want to route to an address in the block 200.23.18.0/23, they will use longest
prefix matching (see Section 4.2.1), and route toward ISPs-R-Us, as it advertises the longest
(i.e., most-specific) address prefix that matches the destination address.
PRINCIPLES IN PRACTICE
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 369
Organization 0
200.23.16.0/23
Organization 1
Fly-By-Night-ISP
“Send me anything
with addresses
beginning
200.23.16.0/20”
ISPs-R-Us
200.23.18.0/23
Organization 2
200.23.20.0/23
Organization 7
200.23.30.0/23
Internet
“Send me anything
with addresses
beginning
199.31.0.0/16”
Figure 4.21 ♦ Hierarchical addressing and route aggregation
Organization 0
200.23.16.0/23
Organization 2
Fly-By-Night-ISP
“Send me anything
with addresses
beginning
200.23.16.0/20”
ISPs-R-Us
200.23.20.0/23
Organization 7
200.23.30.0/23
Organization 1
200.23.18.0/23
Internet
“Send me anything
with addresses
beginning
199.31.0.0/16 or
200.23.18.0/23”
Figure 4.22 ♦ ISPs-R-Us has a more specific route to Organization 1
370 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
We would be remiss if we did not mention yet another type of IP address, the IP
broadcast address 255.255.255.255. When a host sends a datagram with destination
address 255.255.255.255, the message is delivered to all hosts on the same subnet.
Routers optionally forward the message into neighboring subnets as well (although
they usually don’t).
Having now studied IP addressing in detail, we need to know how hosts and
subnets get their addresses in the first place. Let’s begin by looking at how an
organization gets a block of addresses for its devices, and then look at how a
device (such as a host) is assigned an address from within the organization’s block
of addresses.
Obtaining a Block of Addresses
In order to obtain a block of IP addresses for use within an organization’s subnet,
a network administrator might first contact its ISP, which would provide addresses
from a larger block of addresses that had already been allocated to the ISP. For
example, the ISP may itself have been allocated the address block 200.23.16.0/20.
The ISP, in turn, could divide its address block into eight equal-sized contiguous
address blocks and give one of these address blocks out to each of up to eight organizations
that are supported by this ISP, as shown below. (We have underlined the
subnet part of these addresses for your convenience.)
ISP’s block: 200.23.16.0/20 11001000 00010111 00010000 00000000
Organization 0 200.23.16.0/23 11001000 00010111 00010000 00000000
Organization 1 200.23.18.0/23 11001000 00010111 00010010 00000000
Organization 2 200.23.20.0/23 11001000 00010111 00010100 00000000
… … …
Organization 7 200.23.30.0/23 11001000 00010111 00011110 00000000
While obtaining a set of addresses from an ISP is one way to get a block of
addresses, it is not the only way. Clearly, there must also be a way for the ISP itself
to get a block of addresses. Is there a global authority that has ultimate responsibility
for managing the IP address space and allocating address blocks to ISPs and other
organizations? Indeed there is! IP addresses are managed under the authority of the
Internet Corporation for Assigned Names and Numbers (ICANN) [ICANN 2020],
based on guidelines set forth in [RFC 7020]. The role of the nonprofit ICANN organization
is not only to allocate IP addresses, but also to manage the DNS root servers.
It also has the very contentious job of assigning domain names and resolving domain
name disputes. The ICANN allocates addresses to regional Internet registries (for
example, ARIN, RIPE, APNIC, and LACNIC, which together form the Address
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 371
Supporting Organization of ICANN [ASO-ICANN 2020]), and handle the allocation/
management of addresses within their regions.
Obtaining a Host Address: The Dynamic Host Configuration Protocol
Once an organization has obtained a block of addresses, it can assign individual
IP addresses to the host and router interfaces in its organization. A system administrator
will typically manually configure the IP addresses into the router (often
remotely, with a network management tool). Host addresses can also be configured
manually, but typically this is done using the Dynamic Host Configuration
Protocol (DHCP) [RFC 2131]. DHCP allows a host to obtain (be allocated) an
IP address automatically. A network administrator can configure DHCP so that a
given host receives the same IP address each time it connects to the network, or a
host may be assigned a temporary IP address that will be different each time the
host connects to the network. In addition to host IP address assignment, DHCP also
allows a host to learn additional information, such as its subnet mask, the address
of its first-hop router (often called the default gateway), and the address of its local
DNS server.
Because of DHCP’s ability to automate the network-related aspects of connecting
a host into a network, it is often referred to as a plug-and-play or zeroconf
(zero-configuration) protocol. This capability makes it very attractive to the network
administrator who would otherwise have to perform these tasks manually! DHCP
is also enjoying widespread use in residential Internet access networks, enterprise
networks, and in wireless LANs, where hosts join and leave the network frequently.
Consider, for example, the student who carries a laptop from a dormitory room to
a library to a classroom. It is likely that in each location, the student will be connecting
into a new subnet and hence will need a new IP address at each location.
DHCP is ideally suited to this situation, as there are many users coming and going,
and addresses are needed for only a limited amount of time. The value of DHCP’s
plug-and-play capability is clear, since it’s unimaginable that a system administrator
would be able to reconfigure laptops at each location, and few students (except those
taking a computer networking class!) would have the expertise to configure their
laptops manually.
DHCP is a client-server protocol. A client is typically a newly arriving host
wanting to obtain network configuration information, including an IP address for
itself. In the simplest case, each subnet (in the addressing sense of Figure 4.20) will
have a DHCP server. If no server is present on the subnet, a DHCP relay agent (typically
a router) that knows the address of a DHCP server for that network is needed.
Figure 4.23 shows a DHCP server attached to subnet 223.1.2/24, with the router
serving as the relay agent for arriving clients attached to subnets 223.1.1/24 and
223.1.3/24. In our discussion below, we’ll assume that a DHCP server is available
on the subnet.
372 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
For a newly arriving host, the DHCP protocol is a four-step process, as shown in
Figure 4.24 for the network setting shown in Figure 4.23. In this figure, yiaddr (as
in “your Internet address”) indicates the address being allocated to the newly arriving
client. The four steps are:
• DHCP server discovery. The first task of a newly arriving host is to find a DHCP
server with which to interact. This is done using a DHCP discover message,
which a client sends within a UDP packet to port 67. The UDP packet is encapsulated
in an IP datagram. But to whom should this datagram be sent? The host
doesn’t even know the IP address of the network to which it is attaching, much
less the address of a DHCP server for this network. Given this, the DHCP client
creates an IP datagram containing its DHCP discover message along with the
broadcast destination IP address of 255.255.255.255 and a “this host” source IP
address of 0.0.0.0. The DHCP client passes the IP datagram to the link layer,
which then broadcasts this frame to all nodes attached to the subnet (we will cover
the details of link-layer broadcasting in Section 6.4).
• DHCP server offer(s). A DHCP server receiving a DHCP discover message
responds to the client with a DHCP offer message that is broadcast to all
223.1.1.1
223.1.1.2
223.1.1.4 223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1 223.1.3.2
223.1.2.1
223.1.2.5
223.1.2.2
Arriving
DHCP
client
DHCP
server
Figure 4.23 ♦ DHCP client and server
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 373
DHCP server:
223.1.2.5
Arriving client
DHCP discover
Time Time
src: 0.0.0.0, 68
dest: 255.255.255.255,67
DHCPDISCOVER
yiaddr: 0.0.0.0
transaction ID: 654
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPOFFER
yiaddrr: 223.1.2.4
transaction ID: 654
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP offer
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPACK
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP ACK
src: 0.0.0.0, 68
dest: 255.255.255.255, 67
DHCPREQUEST
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP request
Figure 4.24 ♦ DHCP client-server interaction
nodes on the subnet, again using the IP broadcast address of 255.255.255.255.
(You might want to think about why this server reply must also be broadcast).
Since several DHCP servers can be present on the subnet, the client may find
itself in the enviable position of being able to choose from among several
offers. Each server offer message contains the transaction ID of the received
discover message, the proposed IP address for the client, the network mask,
and an IP address lease time—the amount of time for which the IP address
will be valid. It is common for the server to set the lease time to several hours
or days [Droms 2002].
374 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
• DHCP request. The newly arriving client will choose from among one or more
server offers and respond to its selected offer with a DHCP request message,
echoing back the configuration parameters.
• DHCP ACK. The server responds to the DHCP request message with a DHCP
ACK message, confirming the requested parameters.
Once the client receives the DHCP ACK, the interaction is complete and the
client can use the DHCP-allocated IP address for the lease duration. Since a client
may want to use its address beyond the lease’s expiration, DHCP also provides a
mechanism that allows a client to renew its lease on an IP address.
From a mobility aspect, DHCP does have one very significant shortcoming.
Since a new IP address is obtained from DHCP each time a node connects to a
new subnet, a TCP connection to a remote application cannot be maintained as a
mobile node moves between subnets. In Chapter 7, we will learn how mobile cellular
networks allow a host to retain its IP address and ongoing TCP connections as
it moves between base stations in a provider’s cellular network. Additional details
about DHCP can be found in [Droms 2002] and [dhc 2020]. An open source reference
implementation of DHCP is available from the Internet Systems Consortium
[ISC 2020].
4.3.3 Network Address Translation (NAT)
Given our discussion about Internet addresses and the IPv4 datagram format,
we’re now well aware that every IP-capable device needs an IP address. With the
proliferation
of small office, home office (SOHO) subnets, this would seem to imply
that whenever a SOHO wants to install a LAN to connect multiple machines, a range
of addresses would need to be allocated by the ISP to cover all of the SOHO’s IP
devices (including phones, tablets, gaming devices, IP TVs, printers and more).
If the subnet grew bigger, a larger block of addresses would have to be allocated.
But what if the ISP had already allocated the contiguous portions of the SOHO
network’s
current address range? And what typical homeowner wants (or should
need) to know how to manage IP addresses in the first place? Fortunately, there
is a simpler approach to address allocation that has found increasingly widespread
use in such scenarios: network address translation (NAT) [RFC 2663; RFC 3022;
Huston 2004, Zhang 2007; Huston 2017].
Figure 4.25 shows the operation of a NAT-enabled router. The NAT-enabled
router, residing in the home, has an interface that is part of the home network on
the right of Figure 4.25. Addressing within the home network is exactly as we
have seen above—all four interfaces in the home network have the same subnet
address of 10.0.0.0/24. The address space 10.0.0.0/8 is one of three portions of
the IP address space that is reserved in [RFC 1918] for a private network or a
realm with private addresses, such as the home network in Figure 4.25. A realm
with private addresses refers to a network whose addresses only have meaning to
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 375
devices within that network. To see why this is important, consider the fact that
there are hundreds of thousands of home networks, many using the same address
space, 10.0.0.0/24. Devices within a given home network can send packets to each
other using 10.0.0.0/24 addressing. However, packets forwarded beyond the home
network into the larger global Internet clearly cannot use these addresses (as either
a source or a destination address) because there are hundreds of thousands of networks
using this block of addresses. That is, the 10.0.0.0/24 addresses can only
have meaning within the given home network. But if private addresses only have
meaning within a given network, how is addressing handled when packets are sent
to or received from the global Internet, where addresses are necessarily unique? The
answer lies in understanding NAT.
The NAT-enabled router does not look like a router to the outside world. Instead
the NAT router behaves to the outside world as a single device with a single IP
address. In Figure 4.25, all traffic leaving the home router for the larger Internet has
a source IP address of 138.76.29.7, and all traffic entering the home router must have a
destination address of 138.76.29.7. In essence, the NAT-enabled router is hiding
the details of the home network from the outside world. (As an aside, you might
wonder where the home network computers get their addresses and where the router
gets its single IP address. Often, the answer is the same—DHCP! The router gets its
address from the ISP’s DHCP server, and the router runs a DHCP server to provide
addresses to computers within the NAT-DHCP-router-controlled home network’s
address space.)
3
2
10.0.0.1
138.76.29.7
10.0.0.4 10.0.0.2
10.0.0.3
NAT translation table
WAN side
138.76.29.7, 5001
LAN side
10.0.0.1, 3345
. . . . . .
S = 138.76.29.7, 5001
D = 128.119.40.186, 80
1
4
S = 128.119.40.186, 80
D = 138.76.29.7, 5001
S = 128.119.40.186, 80
D = 10.0.0.1, 3345
S = 10.0.0.1, 3345
D = 128.119.40.186, 80
Figure 4.25 ♦ Network address translation
376 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
If all datagrams arriving at the NAT router from the WAN have the same destination
IP address (specifically, that of the WAN-side interface of the NAT router),
then how does the router know the internal host to which it should forward a given
datagram? The trick is to use a NAT translation table at the NAT router, and to
include port numbers as well as IP addresses in the table entries.
Consider the example in Figure 4.25. Suppose a user sitting in a home network
behind host 10.0.0.1 requests a Web page on some Web server (port 80)
with IP address 128.119.40.186. The host 10.0.0.1 assigns the (arbitrary) source
port number 3345 and sends the datagram into the LAN. The NAT router receives
the datagram, generates a new source port number 5001 for the datagram, replaces
the source IP address with its WAN-side IP address 138.76.29.7, and replaces the
original source port number 3345 with the new source port number 5001. When
generating a new source port number, the NAT router can select any source port
number that is not currently in the NAT translation table. (Note that because a port
number field is 16 bits long, the NAT protocol can support over 60,000 simultaneous
connections with a single WAN-side IP address for the router!) NAT
in the router also adds an entry to its NAT translation table. The Web server,
blissfully unaware that the arriving datagram containing the HTTP request has
been manipulated by the NAT router, responds with a datagram whose destination
address is the IP address of the NAT router, and whose destination port number is
5001. When this datagram arrives at the NAT router, the router indexes the NAT
translation table using the destination IP address and destination port number to
obtain the appropriate IP address (10.0.0.1) and destination port number (3345)
for the browser in the home network. The router then rewrites the datagram’s
destination address and destination port number, and forwards the datagram into
the home network.
NAT has enjoyed widespread deployment in recent years. But NAT is
not without detractors. First, one might argue that, port numbers are meant to
be used for addressing processes, not for addressing hosts. This violation can
indeed cause problems for servers running on the home network, since, as we
have seen in Chapter 2, server processes wait for incoming requests at wellknown
port numbers and peers in a P2P protocol need to accept incoming connections
when acting as servers. How can one peer connect to another peer that
is behind a NAT server, and has a DHCP-provided NAT address? Technical
solutions to these problems include NAT traversal tools [RFC 5389] [RFC
5389, RFC 5128, Ford 2005].
More “philosophical” arguments have also been raised against NAT by
architectural purists. Here, the concern is that routers are meant to be layer 3
(i.e., network-layer) devices, and should process packets only up to the network
layer. NAT violates this principle that hosts should be talking directly
with each other, without interfering nodes modifying IP addresses, much less
port numbers. We’ll return to this debate later in Section 4.5, when we cover
middleboxes.
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 377
4.3.4 IPv6
In the early 1990s, the Internet Engineering Task Force began an effort to develop a
successor to the IPv4 protocol. A prime motivation for this effort was the realization
that the 32-bit IPv4 address space was beginning to be used up, with new subnets
INSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION SYSTEMS
Suppose you are assigned the task of administering a home, departmental, university, or
corporate network. Attackers, knowing the IP address range of your network, can easily
send IP datagrams to addresses in your range. These datagrams can do all kinds of
devious things, including mapping your network with ping sweeps and port scans,
crashing vulnerable hosts with malformed packets, scanning for open TCP/UDP ports on
servers in your network, and infecting hosts by including malware in the packets. As the
network administrator, what are you going to do about all those bad guys out there, each
capable of sending malicious packets into your network? Two popular defense mechanisms
to malicious packet attacks are firewalls and intrusion detection systems (IDSs).
As a network administrator, you may first try installing a firewall between your
network and the Internet. (Most access routers today have firewall capability.)
Firewalls inspect the datagram and segment header fields, denying suspicious datagrams
entry into the internal network. For example, a firewall may be configured to
block all ICMP echo request packets (see Section 5.6), thereby preventing an attacker
from doing a traditional port scan across your IP address range. Firewalls can
also block packets based on source and destination IP addresses and port numbers.
Additionally, firewalls can be configured to track TCP connections, granting entry
only to datagrams that belong to approved connections.
Additional protection can be provided with an IDS. An IDS, typically situated at
the network boundary, performs “deep packet inspection,” examining not only header
fields but also the payloads in the datagram (including application-layer data).
An IDS has a database of packet signatures that are known to be part of attacks.
This database is automatically updated as new attacks are discovered. As packets
pass through the IDS, the IDS attempts to match header fields and payloads to the
signatures in its signature database. If such a match is found, an alert is created. An
intrusion prevention system (IPS) is similar to an IDS, except that it actually blocks
packets in addition to creating alerts. We’ll explore firewalls and IDSs in more detail
in Section 4.5 and in again Chapter 8.
Can firewalls and IDSs fully shield your network from all attacks? The answer is
clearly no, as attackers continually find new attacks for which signatures are not yet
available. But firewalls and traditional signature-based IDSs are useful in protecting
your network from known attacks.
FOCUS ON SECURITY
378 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
and IP nodes being attached to the Internet (and being allocated unique IP addresses)
at a breathtaking rate. To respond to this need for a large IP address space, a new
IP protocol, IPv6, was developed. The designers of IPv6 also took this opportunity
to tweak and augment other aspects of IPv4, based on the accumulated operational
experience with IPv4.
The point in time when IPv4 addresses would be completely allocated (and
hence no new networks could attach to the Internet) was the subject of considerable
debate. The estimates of the two leaders of the IETF’s Address Lifetime Expectations
working group were that addresses would become exhausted in 2008 and
2018, respectively [Solensky 1996]. In February 2011, IANA allocated out the last
remaining pool of unassigned IPv4 addresses to a regional registry. While these registries
still have available IPv4 addresses within their pool, once these addresses are
exhausted, there are no more available address blocks that can be allocated from a
central pool [Huston 2011a]. A recent survey of IPv4 address-space exhaustion, and
the steps taken to prolong the life of the address space is [Richter 2015]; a recent
analysis of IPv4 address use is [Huston 2019].
Although the mid-1990s estimates of IPv4 address depletion suggested that a
considerable amount of time might be left until the IPv4 address space was exhausted,
it was realized that considerable time would be needed to deploy a new technology
on such an extensive scale, and so the process to develop IP version 6 (IPv6) [RFC
2460] was begun [RFC 1752]. (An often-asked question is what happened to IPv5?
It was initially envisioned that the ST-2 protocol would become IPv5, but ST-2 was
later dropped.) An excellent source of information about IPv6 is [Huitema 1998].
IPv6 Datagram Format
The format of the IPv6 datagram is shown in Figure 4.26. The most important
changes introduced in IPv6 are evident in the datagram format:
• Expanded addressing capabilities. IPv6 increases the size of the IP address from
32 to 128 bits. This ensures that the world won’t run out of IP addresses. Now,
every grain of sand on the planet can be IP-addressable. In addition to unicast and
multicast addresses, IPv6 has introduced a new type of address, called an anycast
address, that allows a datagram to be delivered to any one of a group of hosts.
(This feature could be used, for example, to send an HTTP GET to the nearest of
a number of mirror sites that contain a given document.)
• A streamlined 40-byte header. As discussed below, a number of IPv4 fields have
been dropped or made optional. The resulting 40-byte fixed-length header allows
for faster processing of the IP datagram by a router. A new encoding of options
allows for more flexible options processing.
• Flow labeling. IPv6 has an elusive definition of a flow. RFC 2460 states that this
allows “labeling of packets belonging to particular flows for which the sender
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 379
requests special handling, such as a non-default quality of service or real-time
service.” For example, audio and video transmission might likely be treated as
a flow. On the other hand, the more traditional applications, such as file transfer
and e-mail, might not be treated as flows. It is possible that the traffic carried by a
high-priority user (for example, someone paying for better service for their traffic)
might also be treated as a flow. What is clear, however, is that the designers of
IPv6 foresaw the eventual need to be able to differentiate among the flows, even
if the exact meaning of a flow had yet to be determined.
As noted above, a comparison of Figure 4.26 with Figure 4.17 reveals the simpler,
more streamlined structure of the IPv6 datagram. The following fields are
defined in IPv6:
• Version. This 4-bit field identifies the IP version number. Not surprisingly, IPv6
carries a value of 6 in this field. Note that putting a 4 in this field does not create
a valid IPv4 datagram. (If it did, life would be a lot simpler—see the discussion
below regarding the transition from IPv4 to IPv6.)
• Traffic class. The 8-bit traffic class field, like the TOS field in IPv4, can be used
to give priority to certain datagrams within a flow, or it can be used to give priority
to datagrams from certain applications (for example, voice-over-IP) over
datagrams from other applications (for example, SMTP e-mail).
• Flow label. As discussed above, this 20-bit field is used to identify a flow of datagrams.
• Payload length. This 16-bit value is treated as an unsigned integer giving the
number of bytes in the IPv6 datagram following the fixed-length, 40-byte datagram
header.
Version Traffic class
Payload length Next hdr Hop limit
Flow label
32 bits
Source address
(128 bits)
Destination address
(128 bits)
Data
Figure 4.26 ♦ IPv6 datagram format
380 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
• Next header. This field identifies the protocol to which the contents (data field) of
this datagram will be delivered (for example, to TCP or UDP). The field uses the
same values as the protocol field in the IPv4 header.
• Hop limit. The contents of this field are decremented by one by each router that
forwards the datagram. If the hop limit count reaches zero, a router must discard
that datagram.
• Source and destination addresses. The various formats of the IPv6 128-bit address
are described in RFC 4291.
• Data. This is the payload portion of the IPv6 datagram. When the datagram
reaches its destination, the payload will be removed from the IP datagram and
passed on to the protocol specified in the next header field.
The discussion above identified the purpose of the fields that are included in the
IPv6 datagram. Comparing the IPv6 datagram format in Figure 4.26 with the IPv4
datagram format that we saw in Figure 4.17, we notice that several fields appearing
in the IPv4 datagram are no longer present in the IPv6 datagram:
• Fragmentation/reassembly. IPv6 does not allow for fragmentation and reassembly
at intermediate routers; these operations can be performed only by the source
and destination. If an IPv6 datagram received by a router is too large to be forwarded
over the outgoing link, the router simply drops the datagram and sends a
“Packet Too Big” ICMP error message (see Section 5.6) back to the sender. The
sender can then resend the data, using a smaller IP datagram size. Fragmentation
and reassembly is a time-consuming operation; removing this functionality from
the routers and placing it squarely in the end systems considerably speeds up IP
forwarding within the network.
• Header checksum. Because the transport-layer (for example, TCP and UDP) and
link-layer (for example, Ethernet) protocols in the Internet layers perform checksumming,
the designers of IP probably felt that this functionality was sufficiently
redundant in the network layer that it could be removed. Once again, fast processing
of IP packets was a central concern. Recall from our discussion of IPv4
in Section 4.3.1 that since the IPv4 header contains a TTL field (similar to the
hop limit field in IPv6), the IPv4 header checksum needed to be recomputed at
every router. As with fragmentation and reassembly, this too was a costly operation
in IPv4.
• Options. An options field is no longer a part of the standard IP header. However,
it has not gone away. Instead, the options field is one of the possible next
headers pointed to from within the IPv6 header. That is, just as TCP or UDP
protocol headers can be the next header within an IP packet, so too can an
options field. The removal of the options field results in a fixed-length, 40-byte
IP header.
4.3 • The Internet Protocol (IP): IPv4, Addressing, IPv6, and More 381
Transitioning from IPv4 to IPv6
Now that we have seen the technical details of IPv6, let us consider a very practical
matter: How will the public Internet, which is based on IPv4, be transitioned to
IPv6? The problem is that while new IPv6-capable systems can be made backwardcompatible,
that is, can send, route, and receive IPv4 datagrams, already deployed
IPv4-capable systems are not capable of handling IPv6 datagrams. Several options
are possible [Huston 2011b, RFC 4213].
One option would be to declare a flag day—a given time and date when all
Internet machines would be turned off and upgraded from IPv4 to IPv6. The last
major technology transition (from using NCP to using TCP for reliable transport
service) occurred almost 40 years ago. Even back then [RFC 801], when the Internet
was tiny and still being administered by a small number of “wizards,” it was realized
that such a flag day was not possible. A flag day involving billions of devices
is even more unthinkable today.
The approach to IPv4-to-IPv6 transition that has been most widely adopted in
practice involves tunneling [RFC 4213]. The basic idea behind tunneling—a key
concept with applications in many other scenarios beyond IPv4-to-IPv6 transition,
including wide use in the all-IP cellular networks that we’ll cover in Chapter 7—is
the following. Suppose two IPv6 nodes (in this example, B and E in Figure 4.27)
want to interoperate using IPv6 datagrams but are connected to each other by intervening
IPv4 routers. We refer to the intervening set of IPv4 routers between two
IPv6 routers as a tunnel, as illustrated in Figure 4.27. With tunneling, the IPv6 node
on the sending side of the tunnel (in this example, B) takes the entire IPv6 datagram
and puts it in the data (payload) field of an IPv4 datagram. This IPv4 datagram is
then addressed to the IPv6 node on the receiving side of the tunnel (in this example,
E) and sent to the first node in the tunnel (in this example, C). The intervening IPv4
routers in the tunnel route this IPv4 datagram among themselves, just as they would
any other datagram, blissfully unaware that the IPv4 datagram itself contains a complete
IPv6 datagram. The IPv6 node on the receiving side of the tunnel eventually
receives the IPv4 datagram (it is the destination of the IPv4 datagram!), determines
that the IPv4 datagram contains an IPv6 datagram (by observing that the protocol
number field in the IPv4 datagram is 41 [RFC 4213], indicating that the IPv4
payload is a IPv6 datagram), extracts the IPv6 datagram, and then routes the IPv6
datagram exactly as it would if it had received the IPv6 datagram from a directly
connected IPv6 neighbor.
We end this section by noting that while the adoption of IPv6 was initially slow
to take off [Lawton 2001; Huston 2008b], momentum has been building. NIST
[NIST IPv6 2020] reports that more than a third of US government second-level
domains are IPv6-enabled. On the client side, Google reports that about 25 percent
of the clients accessing Google services do so via IPv6 [Google IPv6 2020]. Other
recent measurements [Czyz 2014] indicate that IPv6 adoption has been accelerating.
The proliferation of devices such as IP-enabled phones and other portable devices
382 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
provides an additional push for more widespread deployment of IPv6. Europe’s
Third Generation Partnership Program [3GPP 2020] has specified IPv6 as the standard
addressing scheme for mobile multimedia.
One important lesson that we can learn from the IPv6 experience is that it is enormously
difficult to change network-layer protocols. Since the early 1990s, numerous
new network-layer protocols have been trumpeted as the next major revolution for
the Internet, but most of these protocols have had limited penetration to date. These
protocols include IPv6, multicast protocols, and resource reservation protocols; a discussion
of these latter two classes of protocols can be found in the online supplement
to this text. Indeed, introducing new protocols into the network layer is like replacing
the foundation of a house—it is difficult to do without tearing the whole house
down or at least temporarily relocating the house’s residents. On the other hand, the
Internet has witnessed rapid deployment of new protocols at the application layer.
The classic examples, of course, are the Web, instant messaging, streaming media,
distributed games, and various forms of social media. Introducing new applicationlayer
protocols is like adding a new layer of paint to a house—it is relatively easy to
do, and if you choose an attractive color, others in the neighborhood will copy you.
A B C D E F
IPv6
A to B: IPv6
Physical view
B to C: IPv4
(encapsulating IPv6)
D to E: IPv4
(encapsulating IPv6)
E to F: IPv6
IPv6 IPv4 IPv4 IPv6 IPv6
Flow: X
Source: A
Dest: F
data
Source: B
Dest: E
Source: B
Dest: E
A B E F
IPv6
Logical view
IPv6
Tunnel
IPv6 IPv6
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Figure 4.27 ♦ Tunneling
4.4 • General ized Forward ing and SDN 383
In summary, in the future, we can certainly expect to see changes in the Internet’s
network layer, but these changes will likely occur on a time scale that is much slower
than the changes that will occur at the application layer.
4.4 Generalized Forwarding and SDN
Recall that Section 4.2.1 characterized destination-based forwarding as the two steps
of looking up a destination IP address (“match”), then sending the packet into the
switching fabric to the specified output port (“action”). Let’s now consider a significantly
more general “match-plus-action” paradigm, where the “match” can be made
over multiple header fields associated with different protocols at different layers in
the protocol stack. The “action” can include forwarding the packet to one or more
output ports (as in destination-based forwarding), load balancing packets across
multiple outgoing interfaces that lead to a service (as in load balancing), rewriting
header values (as in NAT), purposefully blocking/dropping a packet (as in a firewall),
sending a packet to a special server for further processing and action (as in
DPI), and more.
In generalized forwarding, a match-plus-action table generalizes the notion of
the destination-based forwarding table that we encountered in Section 4.2.1. Because
forwarding decisions may be made using network-layer and/or link-layer source
and destination addresses, the forwarding devices shown in Figure 4.28 are more
accurately described as “packet switches” rather than layer 3 “routers” or layer 2
“switches.” Thus, in the remainder of this section, and in Section 5.5, we’ll refer
to these devices as packet switches, adopting the terminology that is gaining widespread
adoption in SDN literature.
Figure 4.28 shows a match-plus-action table in each packet switch, with the
table being computed, installed, and updated by a remote controller. We note that
while it is possible for the control components at the individual packet switches to
interact with each other (e.g., in a manner similar to that in Figure 4.2), in practice,
generalized match-plus-action capabilities are implemented via a remote controller
that computes, installs, and updates these tables. You might take a minute to compare
Figures 4.2, 4.3, and 4.28—what similarities and differences do you notice between
destination-based forwarding shown in Figures 4.2 and 4.3, and generalized forwarding
shown in Figure 4.28?
Our following discussion of generalized forwarding will be based on Open-
Flow [McKeown 2008, ONF 2020, Casado 2014, Tourrilhes 2014]—a highly visible
standard that has pioneered the notion of the match-plus-action forwarding abstraction
and controllers, as well as the SDN revolution more generally [Feamster 2013].
We’ll primarily consider OpenFlow 1.0, which introduced key SDN abstractions
and functionality in a particularly clear and concise manner. Later versions of
OpenFlow
introduced additional capabilities as a result of experience gained through
384 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
implementation and use; current and earlier versions of the OpenFlow standard can
be found at [ONF 2020].
Each entry in the match-plus-action forwarding table, known as a flow table in
OpenFlow, includes:
• A set of header field values to which an incoming packet will be matched. As in
the case of destination-based forwarding, hardware-based matching is most rapidly
performed in TCAM memory, with more than a million destination address
entries being possible [Bosshart 2013]. A packet that matches no flow table entry
can be dropped or sent to the remote controller for more processing. In practice,
a flow table may be implemented by multiple flow tables for performance or cost
reasons [Bosshart 2013], but we’ll focus here on the abstraction of a single flow
table.
0100 1101
Remote Controller
Values in arriving
packet’s header
1
2
3
Local flow table
...
...
...
...
...
...
...
...
...
...
...
...
Headers Counters Actions
Control plane
Data plane
Figure 4.28 ♦ Generalized forwarding: Each packet switch contains a
match-plus-action table that is computed and distributed
by a remote controller
4.4 • General ized Forward ing and SDN 385
• A set of counters that are updated as packets are matched to flow table entries.
These counters might include the number of packets that have been matched by
that table entry, and the time since the table entry was last updated.
• A set of actions to be taken when a packet matches a flow table entry. These
actions might be to forward the packet to a given output port, to drop the packet,
makes copies of the packet and sent them to multiple output ports, and/or to
rewrite selected header fields.
We’ll explore matching and actions in more detail in Sections 4.4.1 and 4.4.2,
respectively. We’ll then study how the network-wide collection of per-packet switch
matching rules can be used to implement a wide range of functions including routing,
layer-2 switching, firewalling, load-balancing, virtual networks, and more in Section
4.4.3. In closing, we note that the flow table is essentially an API, the abstraction
through which an individual packet switch’s behavior can be programmed;
we’ll see in Section 4.4.3 that network-wide behaviors can similarly be programmed
by appropriately programming/configuring these tables in a collection of network
packet switches [Casado 2014].
4.4.1 Match
Figure 4.29 shows the 11 packet-header fields and the incoming port ID that can
be matched in an OpenFlow 1.0 match-plus-action rule. Recall from Section 1.5.2
that a link-layer (layer 2) frame arriving to a packet switch will contain a network-
layer (layer 3) datagram as its payload, which in turn will typically contain
a transport-layer (layer 4) segment. The first observation we make is that
OpenFlow’s match abstraction allows for a match to be made on selected fields
from three layers of protocol headers (thus rather brazenly defying the layering
principle we studied in Section 1.5). Since we’ve not yet covered the link
layer, suffice it to say that the source and destination MAC addresses shown in
Figure 4.29 are the link-layer addresses associated with the frame’s sending and
receiving interfaces; by forwarding on the basis of Ethernet addresses rather than
IP addresses, we can see that an OpenFlow-enabled device can equally perform
Ingress
Port
Src
MAC
Dst
MAC
Eth
Type
VLAN
ID
VLAN
Pri
IP Src IP Dst
IP
Proto
IP
TOS
TCP/UDP
Src Port
TCP/UDP
Dst Port
Link layer Network layer Transport layer
Figure 4.29 ♦ Packet matching fields, OpenFlow 1.0 flow table
386 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
as a router (layer-3 device) forwarding datagrams as well as a switch (layer-2
device) forwarding frames. The Ethernet type field corresponds to the upper layer
protocol (e.g., IP) to which the frame’s payload will be de-multiplexed, and the
VLAN fields are concerned with so-called virtual local area networks that we’ll
study in Chapter 6. The set of 12 values that can be matched in the OpenFlow
1.0 specification has grown to 41 values in more recent OpenFlow specifications
[Bosshart 2014].
The ingress port refers to the input port at the packet switch on which a packet
is received. The packet’s IP source address, IP destination address, IP protocol field,
and IP type of service fields were discussed earlier in Section 4.3.1. The transport-layer
source and destination port number fields can also be matched.
Flow table entries may also have wildcards. For example, an IP address of
128.119.*.* in a flow table will match the corresponding address field of any datagram
that has 128.119 as the first 16 bits of its address. Each flow table entry also has
an associated priority. If a packet matches multiple flow table entries, the selected
match and corresponding action will be that of the highest priority entry with which
the packet matches.
Lastly, we observe that not all fields in an IP header can be matched. For example
OpenFlow does not allow matching on the basis of TTL field or datagram length
field. Why are some fields allowed for matching, while others are not? Undoubtedly,
the answer has to do with the tradeoff between functionality and complexity. The
“art” in choosing an abstraction is to provide for enough functionality to accomplish
a task (in this case to implement, configure, and manage a wide range of networklayer
functions that had previously been implemented through an assortment of
network-
layer devices), without over-burdening the abstraction with so much detail
and generality that it becomes bloated and unusable. Butler Lampson has famously
noted [Lampson 1983]:
Do one thing at a time, and do it well. An interface should capture the minimum
essentials of an abstraction. Don’t generalize; generalizations are generally
wrong.
Given OpenFlow’s success, one can surmise that its designers indeed chose their
abstraction well. Additional details of OpenFlow matching can be found in [ONF
2020].
4.4.2 Action
As shown in Figure 4.28, each flow table entry has a list of zero or more actions
that determine the processing that is to be applied to a packet that matches a flow
table entry. If there are multiple actions, they are performed in the order specified
in the list.
4.4 • General ized Forward ing and SDN 387
Among the most important possible actions are:
• Forwarding. An incoming packet may be forwarded to a particular physical
output port, broadcast over all ports (except the port on which it arrived) or
multicast over a selected set of ports. The packet may be encapsulated and sent
to the remote controller for this device. That controller then may (or may not)
take some action on that packet, including installing new flow table entries, and
may return the packet to the device for forwarding under the updated set of flow
table rules.
• Dropping. A flow table entry with no action indicates that a matched packet
should be dropped.
• Modify-field. The values in 10 packet-header fields (all layer 2, 3, and 4 fields
shown in Figure 4.29 except the IP Protocol field) may be re-written before the
packet is forwarded to the chosen output port.
4.4.3 OpenFlow Examples of Match-plus-action in Action
Having now considered both the match and action components of generalized
forwarding, let’s put these ideas together in the context of the sample network
shown in Figure 4.30. The network has 6 hosts (h1, h2, h3, h4, h5 and h6) and
three packet switches (s1, s2 and s3), each with four local interfaces (numbered
1 through 4). We’ll consider a number of network-wide behaviors that we’d like
to implement, and the flow table entries in s1, s2 and s3 needed to implement this
behavior.
1
4
s3
s1
s2
2 3
1
2 3
4
Host h6
10.3.0.6
OpenFlow controller
Host h5
10.3.0.5
Host h1
10.1.0.1
Host h2
10.1.0.2
Host h3
10.2.0.3
Host h4
10.2.0.4
1
4
2 3
Figure 4.30 ♦ OpenFlow match-plus-action network with three packet
switches, 6 hosts, and an OpenFlow controller
388 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
A First Example: Simple Forwarding
As a very simple example, suppose that the desired forwarding behavior is that
packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3 to s1, and then
from s1 to s2 (thus completely avoiding the use of the link between s3 and s2). The
flow table entry in s1 would be:
s1 Flow Table (Example 1)
Match Action
Ingress Port = 1 ; IP Src = 10.3.*.* ; IP Dst = 10.2.*.* Forward(4)
… …
Of course, we’ll also need a flow table entry in s3 so that datagrams sent from
h5 or h6 are forwarded to s1 over outgoing interface 3:
s3 Flow Table (Example 1)
Match Action
IP Src = 10.3.*.* ; IP Dst = 10.2.*.* Forward(3)
… …
Lastly, we’ll also need a flow table entry in s2 to complete this first example, so
that datagrams arriving from s1 are forwarded to their destination, either host h3 or h4:
s2 Flow Table (Example 1)
Match Action
Ingress port = 2 ; IP Dst = 10.2.0.3 Forward(3)
Ingress port = 2 ; IP Dst = 10.2.0.4 Forward(4)
… …
A Second Example: Load Balancing
As a second example, let’s consider a load-balancing scenario, where datagrams from
h3 destined to 10.1.*.* are to be forwarded over the direct link between s2 and s1, while
datagrams from h4 destined to 10.1.*.* are to be forwarded over the link between s2
and s3 (and then from s3 to s1). Note that this behavior couldn’t be achieved with IP’s
destination-based forwarding. In this case, the flow table in s2 would be:
4.4 • General ized Forward ing and SDN 389
s2 Flow Table (Example 2)
Match Action
Ingress port = 3; IP Dst = 10.1.*.* Forward(2)
Ingress port = 4; IP Dst = 10.1.*.* Forward(1)
… …
Flow table entries are also needed at s1 to forward the datagrams received from
s2 to either h1 or h2; and flow table entries are needed at s3 to forward datagrams
received on interface 4 from s2 over interface 3 toward s1. See if you can figure out
these flow table entries at s1 and s3.
A Third Example: Firewalling
As a third example, let’s consider a firewall scenario in which s2 wants only to
receive (on any of its interfaces) traffic sent from hosts attached to s3.
s2 Flow Table (Example 3)
Match Action
IP Src = 10.3.*.* IP Dst = 10.2.0.3 Forward(3)
IP Src = 10.3.*.* IP Dst = 10.2.0.4 Forward(4)
… …
If there were no other entries in s2’s flow table, then only traffic from 10.3.*.* would
be forwarded to the hosts attached to s2.
Although we’ve only considered a few basic scenarios here, the versatility and
advantages of generalized forwarding are hopefully apparent. In homework problems,
we’ll explore how flow tables can be used to create many different logical
behaviors, including virtual networks—two or more logically separate networks
(each with their own independent and distinct forwarding behavior)—that use the
same physical set of packet switches and links. In Section 5.5, we’ll return to flow
tables when we study the SDN controllers that compute and distribute the flow tables,
and the protocol used for communicating between a packet switch and its controller.
The match-plus-action flow tables that we’ve seen in this section are actually
a limited form of programmability, specifying how a router should forward and
manipulate (e.g., change a header field) a datagram, based on the match between
the datagram’s header values and the matching conditions. One could imagine an
even richer form of programmability—a programming language with higher-level
constructs such as variables, general purpose arithmetic and Boolean operations,
variables, functions, and conditional statements, as well as constructs specifically
390 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
designed for datagram processing at line rate. P4 (Programming Protocol-independent
Packet Processors) [P4 2020] is such a language, and has gained considerable interest
and traction since its introduction five years ago [Bosshart 2014].
4.5 Middleboxes
Routers are the workhorses of the network layer, and in this chapter, we’ve learned
how they accomplish their “bread and butter” job of forwarding IP datagrams toward
their destination. But in this chapter, and in earlier chapters, we’ve also encountered
other network equipment (“boxes”) within the network that sit on the data path
and perform functions other than forwarding. We encountered Web caches in Section
2.2.5; TCP connection splitters in section 3.7; and network address translation
(NAT), firewalls, and intrusion detection systems in Section 4.3.4. We learned in
Section 4.4 that generalized forwarding allows a modern router to easily and naturally
perform firewalling and load balancing with generalized “match plus action”
operations.
In the past 20 years, we’ve seen tremendous growth in such middleboxes, which
RFC 3234 defines as:
“any intermediary box performing functions apart from normal, standard functions
of an IP router on the data path between a source host and destination
host”
We can broadly identify three types of services performed by middleboxes:
• NAT Translation. As we saw in Section 4.3.4, NAT boxes implement private
network addressing, rewriting datagram header IP addresses and port numbers.
• Security Services. Firewalls block traffic based on header-field values or redirect
packets for additional processing, such as deep packet inspection (DPI). Intrusion
Detection Systems (IDS) are able to detect predetermined patterns and filter
packets accordingly. Application-level e-mail filters block e-mails considered to
be junk, phishing or otherwise posing a security threat.
• Performance Enhancement. These middleboxes perform services such as compression,
content caching, and load balancing of service requests (e.g., an HTTP
request, or a search engine query) to one of a set of servers that can provide the
desired service.
Many other middleboxes [RFC 3234] provide capabilities belonging to these three
types of services, in both wired and wireless cellular [Wang 2011] networks.
With the proliferation of middleboxes comes the attendant need to operate,
manage, and upgrade this equipment. Separate specialized hardware boxes, separate
4.5 • MIDDLEBOXES 391
software stacks, and separate management/operation skills translate to significant
operational and capital costs. It is perhaps not surprising then that researchers are
exploring the use of commodity hardware (networking, computing, and storage) with
specialized software built on top of a common software stack—exactly the approach
taken in SDN a decade earlier—to implement these services. This approach has
become known as network function virtualization (NFV) [Mijumbi 2016]. An
alternate approach that has also been explored is to outsource middlebox functionality
to the cloud [Sherry 2012].
For many years, the Internet architecture had a clear separation between the
network layer and the transport/application layers. In these “good old days,” the
network layer consisted of routers, operating within the network core, to forward
datagrams toward their destinations using fields only in the IP datagram header. The
transport and application layers were implemented in hosts operating at the network
edge. Hosts exchanged packets among themselves in transport-layer segments and
application-layer messages. Today’s middleboxes clearly violate this separation: a
NAT box, sitting between a router and host, rewrites network-layer IP addresses and
transport-layer port numbers; an in-network firewall blocks suspect datagrams using
application-layer (e.g., HTTP), transport-layer, and network-layer header fields;
e-mail security gateways are injected between the e-mail sender (whether malicious
or not) and the intended e-mail receiver, filtering application-layer e-mail messages
based on whitelisted/blacklisted IP addresses as well as e-mail message content.
While there are those who have considered such middleboxes as a bit of an architectural
abomination [Garfinkel 2003], others have adopted the philosophy that such
middleboxes “exist for important and permanent reasons”—that they fill an important
need—and that we’ll have more, not fewer, middleboxes in the future [Walfish 2004].
See the section in attached sidebar on “The end-to-end argument” for a slightly different
lens on the question of where to place service functionality in a network.
ARCHITECTURAL PRINCIPLES OF THE INTERNET
Given the phenomenal success of the Internet, one might naturally wonder about the
architectural principles that have guided the development of what is arguably the largest
and most complex engineered system ever built by humankind. RFC 1958, entitled
“Architectural Principles of the Internet,” suggests that these principles, if indeed they exist,
are truly minimal:
“Many members of the Internet community would argue that there is no architecture,
but only a tradition, which was not written down for the first 25 years (or at least not
by the IAB). However, in very general terms, the community believes that the goal is
connectivity, the tool is the Internet Protocol, and the intelligence is end to end rather
than hidden in the network.” [RFC 1958]
PRINCIPLES IN PRACTICE
392 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE
So there we have it! The goal was to provide connectivity, there would be just one network-
layer protocol (the celebrated IP protocol we have studied in this chapter), and “intelligence”
(one might say the “complexity”) would be placed at the network edge, rather than
in the network core. Let’s look these last two considerations in a bit more detail.
THE IP HOURGLASS
By now, we’re well acquainted with the five-layer Internet protocol stack that we first
encountered in Figure 1.23. Another visualization of this stack, shown in Figure 4.31 and
sometimes known as the “IP hourglass,” illustrates the “narrow waist” of the layered
Internet architecture. While the Internet has many protocols in the physical, link, transport,
and application layers, there is only one network layer protocol—the IP protocol. This is
the one protocol that must be implemented by each and every of the billions of Internetconnected
devices. This narrow waist has played a critical role in the phenomenal growth
of the Internet. The relative simplicity of the IP protocol, and the fact that it is the only
universal requirement for Internet connectivity has allowed a rich variety of networks—with
very different underlying link-layer technologies, from Ethernet to WiFi to cellular to optical
networks to become part of the Internet. [Clark 1997] notes that role of the narrow waist,
which he refers to as a “spanning layer,” is to “… hide the detailed differences among
these various [underlying] technologies and present a uniform service interface to the applications
above.” For the IP layer in particular: “How does the IP spanning layer achieve
its purpose? It defines a basic set of services, which were carefully designed so that they
could be constructed from a wide range of underlying network technologies. Software, as
a part of the Internet [i.e., network] layer, translates what each of these lower-layer technologies
offers into the common service of the Internet layer.”
For a discussion the narrow waist, including examples beyond the Internet, see [Beck 2019;
Akhshabi 2011]. We note here that as the Internet architecture enters mid-life (certainly,
IP
TCP UDP
HTTP SMTP
QUIC DASH
RTP …
Ethernet PPP …
PDCP WiFi Bluetooth
copper radio fiber
Figure 4.31 ♦ The narrow-waisted Internet hourglass
4.5 • MIDDLEBOXES 393
the Internet’s age of 40 to 50 years qualifies it for middle age!), one might observe that
its “narrow waist” may indeed be widening a bit (as often happens in middle age!) via
the rise of middleboxes.
THE END-TO-END ARGUMENT
The third principle in RFC 1958—that “intelligence is end to end rather than hidden in the
network”—speaks to the placement of functionality within the network. Here, we’ve seen
that until the recent rise of middleboxes, most Internet functionality was indeed placed at
the network’s edge. It’s worth noting that, in direct contrast with the 20th century telephone
network—which had “dumb” (non-programmable) endpoints and smart switches—the
Internet has always had smart endpoints (programmable computers), enabling complex
functionality to be placed at those endpoints. But a more principled argument for actually
placing functionality at the endpoints was made in an extremely influential paper [Saltzer
1984] that articulated the “end-to-end argument.” It stated:
“ . . . there is a list of functions each of which might be implemented in any of several
ways: by the communication subsystem, by its client, as a joint venture, or perhaps
redundantly, each doing its own version. In reasoning about this choice, the requirements
of the application provide the basis for a class of arguments, which go as follows:
The function in question can completely and correctly be implemented only with
the knowledge and help of the application standing at the end points of the communication
system. Therefore, providing that questioned function as a feature of the
communication system itself is not possible. (Sometimes an incomplete version of the
function provided by the communication system may be useful as a performance
enhancement.)
We call this line of reasoning against low-level function implementation the “end-to-end
argument.”
An example illustrating the end-to-end argument is that of reliable data transfer. Since
packets can be lost within the network (e.g., even without buffer overflows, a router holding
a queued packet could crash, or a portion of the network in which a packet is queued
becomes detached due to link failures), the endpoints (in this case via the TCP protocol)
must perform error control. As we will see in Chapter 6, some link-layer protocols do
indeed perform local error control, but this local error control alone is “incomplete” and
not sufficient to provide end-to-end reliable data transfer. And so reliable data transfer must
be implemented end to end.
RFC 1958 deliberately includes only two references, both of which are “fundamental
papers on the Internet architecture.” One of these is the end-to-end paper itself [Saltzer 1984];
the second paper [Clark 1988] discusses the design philosophy of the DARPA Internet Protocols.
Both are interesting “must reads” for anyone interested in Internet architecture. Follow-ons to
[Clark 1988] are [Blumenthal 2001; Clark 2005] which reconsider Internet architecture in light
of the much more complex environment in which today’s Internet must now operate.
394 CHAPTER 4 • THE NETWORK LAYER: DATA PLANE